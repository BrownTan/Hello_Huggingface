{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何获取模型的数据处理模版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLMTokenizer(name_or_path='/node6_1/tanshuai/ZhipuAI/chatglm3-6b-base', vocab_size=64798, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/node6_1/tanshuai/ZhipuAI/chatglm3-6b-base\", trust_remote_code=True)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbad87c059b4a7095607a84a449abd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLMForConditionalGeneration(\n",
      "  (transformer): ChatGLMModel(\n",
      "    (embedding): Embedding(\n",
      "      (word_embeddings): Embedding(65024, 4096)\n",
      "    )\n",
      "    (rotary_pos_emb): RotaryEmbedding()\n",
      "    (encoder): GLMTransformer(\n",
      "      (layers): ModuleList(\n",
      "        (0-27): 28 x GLMBlock(\n",
      "          (input_layernorm): RMSNorm()\n",
      "          (self_attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)\n",
      "            (core_attention): CoreAttention(\n",
      "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (post_attention_layernorm): RMSNorm()\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)\n",
      "            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layernorm): RMSNorm()\n",
      "    )\n",
      "    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = AutoModel.from_pretrained(\"/node6_1/tanshuai/ZhipuAI/chatglm3-6b-base\", \n",
    "                                  trust_remote_code=True, \n",
    "                                  low_cpu_mem_usage=True,\n",
    "                                  torch_dtype=torch.half,\n",
    "                                  device_map=\"auto\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'name': '考试的技巧有哪些？ 考试的技巧有： 1、 认真审题，仔细答题； 2、 考试中，要合理安排时间，不要在某一题上花费过多时间； 3、 做完题后，要认真检查，避免因为粗心大意而失分； 4、 在考试中，要保持良好的心态，不要因为一时的失败而气馁； 5、 考试前，要提前准备，充分复习，做到心中有数； 6、 考试中，要注意书写规范，不要因为字迹潦草而失分； 7、 考试中，要注意语言表达的准确性和流畅性，不要因为表达不清而失分； 8、 考试中，要注意答题的条理性和逻辑性，不要因为答题不规范而失分； 9、 考试中，要注意细节，不要因为小错误而失分； 10、考试中，要注意时间管理，不要因为时间不够而失分。 以上是考试的技巧，希望大家在考试中能够发挥出色，取得好成绩。',\n",
       "  'content': ''},\n",
       " [{'role': 'user', 'content': '考试的技巧有哪些？'},\n",
       "  {'role': 'assistant',\n",
       "   'metadata': '考试的技巧有哪些？ 考试的技巧有： 1、 认真审题，仔细答题； 2、 考试中，要合理安排时间，不要在某一题上花费过多时间； 3、 做完题后，要认真检查，避免因为粗心大意而失分； 4、 在考试中，要保持良好的心态，不要因为一时的失败而气馁； 5、 考试前，要提前准备，充分复习，做到心中有数； 6、 考试中，要注意书写规范，不要因为字迹潦草而失分； 7、 考试中，要注意语言表达的准确性和流畅性，不要因为表达不清而失分； 8、 考试中，要注意答题的条理性和逻辑性，不要因为答题不规范而失分； 9、 考试中，要注意细节，不要因为小错误而失分； 10、考试中，要注意时间管理，不要因为时间不够而失分。 以上是考试的技巧，希望大家在考试中能够发挥出色，取得好成绩。',\n",
       "   'content': ''}])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "需要将modeling_chatglm源码中的613行部分进行调整，代码如下：\n",
    "\n",
    "```\n",
    "if not kv_caches:\n",
    "    kv_caches = [None for _ in range(self.num_layers)]\n",
    "else:\n",
    "    kv_caches = kv_caches[1]\n",
    "```\n",
    "\n",
    "如果不进行调整，后续chat阶段会报错\n",
    "\"\"\"\n",
    "model.chat(tokenizer, \"考试的技巧有哪些？\", history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhistory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrole\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8192\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.cache/huggingface/modules/transformers_modules/chatglm3-6b-base/modeling_chatglm.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "?model.chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_chat_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'user'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.cache/huggingface/modules/transformers_modules/chatglm3-6b-base/tokenization_chatglm.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "?tokenizer.build_chat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [906, 31007, 4865, 31007, 30994], 'attention_mask': [1, 1, 1, 1, 1], 'position_ids': [0, 1, 2, 3, 4]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<|user|>\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64795"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 特殊字符只有通过get_command才能得到，避免歧义\n",
    "tokenizer.get_command(\"<|user|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[64790, 64792, 64795, 30910,    13, 30910, 32227, 54530, 33741, 34953,\n",
       "         31514, 64796]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.build_chat_input(\"考试的技巧有哪些？\", history=[], role=\"user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[gMASK]sop<|user|> \\n 考试的技巧有哪些？<|assistant|>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出就是数据要处理成的样子\n",
    "tokenizer.decode([64790, 64792, 64795, 30910,    13, 30910, 32227, 54530, 33741, 34953,\n",
    "         31514, 64796])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据格式：\n",
    "\n",
    "[gMASK]sop<|user|> \\n Prompt<|assistant|> \\n Response eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

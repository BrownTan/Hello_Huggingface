{
    "cells": [
     {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2226a82f-b6dc-4450-98d4-c0b1c7a0b7e2",
      "metadata": {},
      "outputs": [],
      "source": [
       "from datasets import *"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "24e49986-8fcb-4e84-af24-156a3ba5212a",
      "metadata": {},
      "source": [
       "## Datasets基本使用"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "0189f65d-1540-4265-8596-98ca08218252",
      "metadata": {},
      "source": [
       "### 加载在线数据集"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7cca5f5b-8b23-4cca-8ed7-688aeebb0c22",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "DatasetDict({\n",
          "    train: Dataset({\n",
          "        features: ['title', 'content'],\n",
          "        num_rows: 5850\n",
          "    })\n",
          "    validation: Dataset({\n",
          "        features: ['title', 'content'],\n",
          "        num_rows: 1679\n",
          "    })\n",
          "})"
         ]
        },
        "execution_count": 3,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "datasets = load_dataset(\"madao33/new-title-chinese\")\n",
       "datasets"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "fcf58d89-ee8e-4a9b-be83-db68c7cc41d9",
      "metadata": {},
      "source": [
       "### 加载数据集合集中的某一项任务（如MMMLU）"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a47b4564-5ba1-4fe3-aa0b-7d15300749f9",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "DatasetDict({\n",
          "    test: Dataset({\n",
          "        features: ['Unnamed: 0', 'Question', 'A', 'B', 'C', 'D', 'Answer', 'Subject'],\n",
          "        num_rows: 14042\n",
          "    })\n",
          "})"
         ]
        },
        "execution_count": 4,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "sub_dataset = load_dataset(\"openai/MMMLU\", \"ZH_CN\")\n",
       "sub_dataset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d1d37954-525c-487a-a0cb-d4238afef92a",
      "metadata": {},
      "outputs": [
       {
        "ename": "DatasetNotFoundError",
        "evalue": "Dataset 'super_plue' doesn't exist on the Hub or cannot be accessed.",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
         "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m boolq_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msuper_plue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboolq\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m boolq_dataset\n",
         "File \u001b[0;32m~/.conda/envs/abc/lib/python3.9/site-packages/datasets/load.py:2074\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2069\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2070\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2071\u001b[0m )\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2074\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
         "File \u001b[0;32m~/.conda/envs/abc/lib/python3.9/site-packages/datasets/load.py:1795\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1793\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1794\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1795\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1804\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1808\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
         "File \u001b[0;32m~/.conda/envs/abc/lib/python3.9/site-packages/datasets/load.py:1659\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[0;32m-> 1659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1661\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trust_remote_code:\n",
         "File \u001b[0;32m~/.conda/envs/abc/lib/python3.9/site-packages/datasets/load.py:1597\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[1;32m   1594\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRevision \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1595\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1597\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hub or cannot be accessed.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_info\u001b[38;5;241m.\u001b[39mgated:\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
         "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'super_plue' doesn't exist on the Hub or cannot be accessed."
        ]
       }
      ],
      "source": [
       "boolq_dataset = load_dataset(\"super_plue\", \"boolq\", trust_remote_code=True)\n",
       "boolq_dataset"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "d3569e6f-ac03-4be3-9622-379dc0e11f09",
      "metadata": {},
      "source": [
       "### 按照数据集划分进行加载"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 6,
      "id": "449585da-230e-48fc-aa64-9296decf4bb1",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "Dataset({\n",
          "    features: ['title', 'content'],\n",
          "    num_rows: 5850\n",
          "})"
         ]
        },
        "execution_count": 6,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dataset = load_dataset(\"madao33/new-title-chinese\", split=\"train\")\n",
       "dataset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 7,
      "id": "422660ac-a53a-4812-a168-45b49fd196f6",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "Dataset({\n",
          "    features: ['title', 'content'],\n",
          "    num_rows: 100\n",
          "})"
         ]
        },
        "execution_count": 7,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dataset = load_dataset(\"madao33/new-title-chinese\", split=\"train[:100]\")\n",
       "dataset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 8,
      "id": "57d9935e-7272-4c3d-b61e-d6da1237b96b",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "Dataset({\n",
          "    features: ['title', 'content'],\n",
          "    num_rows: 2925\n",
          "})"
         ]
        },
        "execution_count": 8,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dataset = load_dataset(\"madao33/new-title-chinese\", split=\"train[:50%]\")\n",
       "dataset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0677dd8a-c563-42fa-90d2-0529f47e36e6",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "[Dataset({\n",
          "     features: ['title', 'content'],\n",
          "     num_rows: 90\n",
          " }),\n",
          " Dataset({\n",
          "     features: ['title', 'content'],\n",
          "     num_rows: 168\n",
          " })]"
         ]
        },
        "execution_count": 9,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dataset = load_dataset(\"madao33/new-title-chinese\", split=[\"train[10:100]\", \"validation[:10%]\"])\n",
       "dataset"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "b5f41226-182e-4709-895c-e9bbbc51ad72",
      "metadata": {},
      "source": [
       "### 查看数据集"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c765eae7-f105-4f47-bfb2-4022c51f282c",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "DatasetDict({\n",
          "    train: Dataset({\n",
          "        features: ['title', 'content'],\n",
          "        num_rows: 5850\n",
          "    })\n",
          "    validation: Dataset({\n",
          "        features: ['title', 'content'],\n",
          "        num_rows: 1679\n",
          "    })\n",
          "})"
         ]
        },
        "execution_count": 10,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "datasets = load_dataset(\"madao33/new-title-chinese\")\n",
       "datasets"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 11,
      "id": "23ec287b-8659-4ade-8132-5aaf641326cb",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'title': '望海楼美国打“台湾牌”是危险的赌博',\n",
          " 'content': '近期，美国国会众院通过法案，重申美国对台湾的承诺。对此，中国外交部发言人表示，有关法案严重违反一个中国原则和中美三个联合公报规定，粗暴干涉中国内政，中方对此坚决反对并已向美方提出严正交涉。\\n事实上，中国高度关注美国国内打“台湾牌”、挑战一中原则的危险动向。近年来，作为“亲台”势力大本营的美国国会动作不断，先后通过“与台湾交往法”“亚洲再保证倡议法”等一系列“挺台”法案，“2019财年国防授权法案”也多处触及台湾问题。今年3月，美参院亲台议员再抛“台湾保证法”草案。众院议员继而在4月提出众院版的草案并在近期通过。上述法案的核心目标是强化美台关系，并将台作为美“印太战略”的重要伙伴。同时，“亲台”议员还有意制造事端。今年2月，5名共和党参议员致信众议院议长，促其邀请台湾地区领导人在国会上发表讲话。这一动议显然有悖于美国与台湾的非官方关系，其用心是实质性改变美台关系定位。\\n上述动向出现并非偶然。在中美建交40周年之际，两国关系摩擦加剧，所谓“中国威胁论”再次沉渣泛起。美国对华认知出现严重偏差，对华政策中负面因素上升，保守人士甚至成立了“当前中国威胁委员会”。在此背景下，美国将台海关系作为战略抓手，通过打“台湾牌”在双边关系中增加筹码。特朗普就任后，国会对总统外交政策的约束力和塑造力加强。其实国会推动通过涉台法案对行政部门不具约束力，美政府在2018年并未提升美台官员互访级别，美军舰也没有“访问”台湾港口，保持着某种克制。但从美总统签署国会通过的法案可以看出，国会对外交产生了影响。立法也为政府对台政策提供更大空间。\\n然而，美国需要认真衡量打“台湾牌”成本。首先是美国应对危机的代价。美方官员和学者已明确发出警告，美国卷入台湾问题得不偿失。美国学者曾在媒体发文指出，如果台海爆发危机，美国可能需要“援助”台湾，进而导致新的冷战乃至与中国大陆的冲突。但如果美国让台湾自己面对，则有损美国的信誉，影响美盟友对同盟关系的支持。其次是对中美关系的危害。历史证明，中美合则两利、斗则两伤。中美关系是当今世界最重要的双边关系之一，保持中美关系的稳定发展，不仅符合两国和两国人民的根本利益，也是国际社会的普遍期待。美国蓄意挑战台湾问题的底线，加剧中美关系的复杂性和不确定性，损害两国在重要领域合作，损人又害己。\\n美国打“台湾牌”是一场危险的赌博。台湾问题是中国核心利益，中国政府和人民决不会对此坐视不理。中国敦促美方恪守一个中国原则和中美三个联合公报规定，阻止美国会审议推进有关法案，妥善处理涉台问题。美国悬崖勒马，才是明智之举。\\n（作者系中国国际问题研究院国际战略研究所副所长）'}"
         ]
        },
        "execution_count": 11,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "datasets[\"train\"][0]"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2ea32e8d-51de-4722-8afc-5d4259e7b892",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'title': ['望海楼美国打“台湾牌”是危险的赌博', '大力推进高校治理能力建设'],\n",
          " 'content': ['近期，美国国会众院通过法案，重申美国对台湾的承诺。对此，中国外交部发言人表示，有关法案严重违反一个中国原则和中美三个联合公报规定，粗暴干涉中国内政，中方对此坚决反对并已向美方提出严正交涉。\\n事实上，中国高度关注美国国内打“台湾牌”、挑战一中原则的危险动向。近年来，作为“亲台”势力大本营的美国国会动作不断，先后通过“与台湾交往法”“亚洲再保证倡议法”等一系列“挺台”法案，“2019财年国防授权法案”也多处触及台湾问题。今年3月，美参院亲台议员再抛“台湾保证法”草案。众院议员继而在4月提出众院版的草案并在近期通过。上述法案的核心目标是强化美台关系，并将台作为美“印太战略”的重要伙伴。同时，“亲台”议员还有意制造事端。今年2月，5名共和党参议员致信众议院议长，促其邀请台湾地区领导人在国会上发表讲话。这一动议显然有悖于美国与台湾的非官方关系，其用心是实质性改变美台关系定位。\\n上述动向出现并非偶然。在中美建交40周年之际，两国关系摩擦加剧，所谓“中国威胁论”再次沉渣泛起。美国对华认知出现严重偏差，对华政策中负面因素上升，保守人士甚至成立了“当前中国威胁委员会”。在此背景下，美国将台海关系作为战略抓手，通过打“台湾牌”在双边关系中增加筹码。特朗普就任后，国会对总统外交政策的约束力和塑造力加强。其实国会推动通过涉台法案对行政部门不具约束力，美政府在2018年并未提升美台官员互访级别，美军舰也没有“访问”台湾港口，保持着某种克制。但从美总统签署国会通过的法案可以看出，国会对外交产生了影响。立法也为政府对台政策提供更大空间。\\n然而，美国需要认真衡量打“台湾牌”成本。首先是美国应对危机的代价。美方官员和学者已明确发出警告，美国卷入台湾问题得不偿失。美国学者曾在媒体发文指出，如果台海爆发危机，美国可能需要“援助”台湾，进而导致新的冷战乃至与中国大陆的冲突。但如果美国让台湾自己面对，则有损美国的信誉，影响美盟友对同盟关系的支持。其次是对中美关系的危害。历史证明，中美合则两利、斗则两伤。中美关系是当今世界最重要的双边关系之一，保持中美关系的稳定发展，不仅符合两国和两国人民的根本利益，也是国际社会的普遍期待。美国蓄意挑战台湾问题的底线，加剧中美关系的复杂性和不确定性，损害两国在重要领域合作，损人又害己。\\n美国打“台湾牌”是一场危险的赌博。台湾问题是中国核心利益，中国政府和人民决不会对此坐视不理。中国敦促美方恪守一个中国原则和中美三个联合公报规定，阻止美国会审议推进有关法案，妥善处理涉台问题。美国悬崖勒马，才是明智之举。\\n（作者系中国国际问题研究院国际战略研究所副所长）',\n",
          "  '在推进“双一流”高校建设进程中，我们要紧紧围绕为党育人、为国育才，找准问题、破解难题，以一流意识和担当精神，大力推进高校的治理能力建设。\\n增强政治引领力。坚持党对高校工作的全面领导，始终把政治建设摆在首位，增强校党委的政治领导力，全面推进党的建设各项工作。落实立德树人根本任务，把培养社会主义建设者和接班人放在中心位置。紧紧抓住思想政治工作这条生命线，全面加强师生思想政治工作，推进“三全育人”综合改革，将思想政治工作贯穿学校教育管理服务全过程，努力让学生成为德才兼备、全面发展的人才。\\n提升人才聚集力。人才是创新的核心要素，创新驱动本质上是人才驱动。要坚持引育并举，建立绿色通道，探索知名专家举荐制，完善“一事一议”支持机制。在大力支持自然科学人才队伍建设的同时，实施哲学社会科学人才工程。立足实际，在条件成熟的学院探索“一院一策”改革。创新科研组织形式，为人才成长创设空间，建设更加崇尚学术、更加追求卓越、更加关爱学生、更加担当有为的学术共同体。\\n培养学生竞争力。遵循学生成长成才的规律培育人才，着力培养具有国际竞争力的拔尖创新人才和各类专门人才，使优势学科、优秀教师、优质资源、优良环境围绕立德树人的根本任务配置。淘汰“水课”，打造“金课”，全力打造世界一流本科教育。深入推进研究生教育综合改革，加强事关国家重大战略的高精尖急缺人才培养，建设具有国际竞争力的研究生教育。\\n激发科技创新力。在国家急需发展的领域挑大梁，就要更加聚焦科技前沿和国家需求，狠抓平台建设，包括加快牵头“武汉光源”建设步伐，积极参与国家实验室建设，建立校级大型科研仪器设备共享平台。关键核心技术领域“卡脖子”问题，归根结底是基础科学研究薄弱。要加大基础研究的支持力度，推进理论、技术和方法创新，鼓励支持重大原创和颠覆性技术创新，催生一批高水平、原创性研究成果。\\n发展社会服务力。在贡献和服务中体现价值，推动合作共建、多元投入的格局，大力推进政产学研用结合，强化科技成果转移转化及产业化。探索校城融合发展、校地联动发展的新模式，深度融入地方创新发展网络，为地方经济社会发展提供人才支撑，不断拓展和优化社会服务网络。\\n涵育文化软实力。加快体制机制改革，优化学校、学部、学院三级评审机制，充分发挥优秀学者特别是德才兼备的年轻学者在学术治理中的重要作用。牢固树立一流意识、紧紧围绕一流目标、认真执行一流标准，让成就一流事业成为普遍追求和行动自觉。培育具有强大凝聚力的大学文化，营造积极团结、向上向善、干事创业的氛围，让大学成为吸引和留住一大批优秀人才建功立业的沃土，让敢干事、肯干事、能干事的人有更多的荣誉感和获得感。\\n建设中国特色、世界一流大学不是等得来、喊得来的，而是脚踏实地拼出来、干出来的。对标一流，深化改革，坚持按章程办学，构建以一流质量标准为核心的制度规范体系，扎实推进学校综合改革，探索更具活力、更富效率的管理体制和运行机制，我们就一定能构建起具有中国特色的现代大学治理体系，进一步提升管理服务水平和工作效能。\\n（作者系武汉大学校长）']}"
         ]
        },
        "execution_count": 12,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "datasets[\"train\"][:2]"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d8c6dbe1-7386-4d62-8334-b18811bd8050",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "['望海楼美国打“台湾牌”是危险的赌博',\n",
          " '大力推进高校治理能力建设',\n",
          " '坚持事业为上选贤任能',\n",
          " '“大朋友”的话儿记心头',\n",
          " '用好可持续发展这把“金钥匙”']"
         ]
        },
        "execution_count": 13,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "datasets[\"train\"][\"title\"][:5]"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 14,
      "id": "94b1abc4-84b1-424a-913c-9c2235e578e1",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "['title', 'content']"
         ]
        },
        "execution_count": 14,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "datasets[\"train\"].column_names"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 15,
      "id": "939f8e71-b3c6-469f-bcc1-68bce3999585",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'title': Value(dtype='string', id=None),\n",
          " 'content': Value(dtype='string', id=None)}"
         ]
        },
        "execution_count": 15,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "datasets[\"train\"].features"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "52d98a1b-6cd1-4283-a0e4-42ad5d1c098b",
      "metadata": {},
      "source": [
       "### 数据集划分"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9f0bb4ea-da8f-4a0b-8f0d-be5150887310",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "DatasetDict({\n",
          "    train: Dataset({\n",
          "        features: ['title', 'content'],\n",
          "        num_rows: 5265\n",
          "    })\n",
          "    test: Dataset({\n",
          "        features: ['title', 'content'],\n",
          "        num_rows: 585\n",
          "    })\n",
          "})"
         ]
        },
        "execution_count": 16,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dataset = datasets[\"train\"]\n",
       "dataset.train_test_split(test_size=0.1)  # 指定测试集比例"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2b36a25f-f1ab-42fc-b571-31aea26c15fa",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "DatasetDict({\n",
          "    train: Dataset({\n",
          "        features: ['question', 'answer', 'passage'],\n",
          "        num_rows: 9427\n",
          "    })\n",
          "    validation: Dataset({\n",
          "        features: ['question', 'answer', 'passage'],\n",
          "        num_rows: 3270\n",
          "    })\n",
          "})"
         ]
        },
        "execution_count": 17,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "boolq_dataset = load_dataset(\"boolq\")\n",
       "boolq_dataset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1ce4ce5f-5dc4-4008-9c67-8391835bdb47",
      "metadata": {},
      "outputs": [
       {
        "ename": "ValueError",
        "evalue": "Stratifying by column is only supported for ClassLabel column, and column answer is Value.",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
         "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m boolq_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify_by_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 保持测试集与整个数据集里“answer”的数据分类比例一致，仅支持ClassLabel类型\u001b[39;00m\n",
         "File \u001b[0;32m~/.conda/envs/abc/lib/python3.9/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
         "File \u001b[0;32m~/.conda/envs/abc/lib/python3.9/site-packages/datasets/fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
         "File \u001b[0;32m~/.conda/envs/abc/lib/python3.9/site-packages/datasets/arrow_dataset.py:4566\u001b[0m, in \u001b[0;36mDataset.train_test_split\u001b[0;34m(self, test_size, train_size, shuffle, stratify_by_column, seed, generator, keep_in_memory, load_from_cache_file, train_indices_cache_file_name, test_indices_cache_file_name, writer_batch_size, train_new_fingerprint, test_new_fingerprint)\u001b[0m\n\u001b[1;32m   4564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstratify_by_column\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures[stratify_by_column], ClassLabel):\n\u001b[0;32m-> 4566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4567\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStratifying by column is only supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mClassLabel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m column, and column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstratify_by_column\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures[stratify_by_column])\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4568\u001b[0m     )\n\u001b[1;32m   4569\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   4570\u001b[0m     train_indices, test_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   4571\u001b[0m         stratified_shuffle_split_generate_indices(\n\u001b[1;32m   4572\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_format(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m)[stratify_by_column], n_train, n_test, rng\u001b[38;5;241m=\u001b[39mgenerator\n\u001b[1;32m   4573\u001b[0m         )\n\u001b[1;32m   4574\u001b[0m     )\n",
         "\u001b[0;31mValueError\u001b[0m: Stratifying by column is only supported for ClassLabel column, and column answer is Value."
        ]
       }
      ],
      "source": [
       "dataset = boolq_dataset[\"train\"]\n",
       "dataset.train_test_split(test_size=0.1, stratify_by_column=\"answer\")  # 保持测试集与整个数据集里“answer”的数据分类比例一致，仅支持ClassLabel类型"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f8215f5d-b0f9-452f-8177-0282846675e4",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'question': Value(dtype='string', id=None),\n",
          " 'answer': Value(dtype='bool', id=None),\n",
          " 'passage': Value(dtype='string', id=None)}"
         ]
        },
        "execution_count": 19,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dataset.features"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a5a31e83-1549-41ed-ab81-f664181cc5cc",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "datasets.features.features.ClassLabel"
         ]
        },
        "execution_count": 20,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "ClassLabel"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 21,
      "id": "2c8d12c4-ce74-4a56-8927-43dcd423da8a",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "DatasetDict({\n",
          "    train: Dataset({\n",
          "        features: ['text1', 'text2', 'target', 'feat_idx', 'evaluation_predictions'],\n",
          "        num_rows: 408\n",
          "    })\n",
          "})"
         ]
        },
        "execution_count": 21,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "mrpc_dataset = load_dataset(\"autoevaluate/autoeval-staging-eval-glue-mrpc-71a11b-14455978\")\n",
       "mrpc_dataset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 22,
      "id": "9c75cfbd-9a88-4255-8fad-a3bb4af44cfc",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'text1': Value(dtype='string', id=None),\n",
          " 'text2': Value(dtype='string', id=None),\n",
          " 'target': ClassLabel(names=['equivalent', 'not_equivalent'], id=None),\n",
          " 'feat_idx': Value(dtype='int32', id=None),\n",
          " 'evaluation_predictions': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None)}"
         ]
        },
        "execution_count": 22,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "mrpc_dataset[\"train\"].features"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 23,
      "id": "9606de21-0aa5-441e-b482-b9f19c5d8628",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "DatasetDict({\n",
          "    train: Dataset({\n",
          "        features: ['text1', 'text2', 'target', 'feat_idx', 'evaluation_predictions'],\n",
          "        num_rows: 367\n",
          "    })\n",
          "    test: Dataset({\n",
          "        features: ['text1', 'text2', 'target', 'feat_idx', 'evaluation_predictions'],\n",
          "        num_rows: 41\n",
          "    })\n",
          "})"
         ]
        },
        "execution_count": 23,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dataset = mrpc_dataset[\"train\"]\n",
       "dataset.train_test_split(test_size=0.1, stratify_by_column=\"target\")  # 终于找到一个ClassLabel类型"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "36f63cf8-7eb8-4827-8259-0eeb0e461bd4",
      "metadata": {},
      "source": [
       "### 数据选取与过滤"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 24,
      "id": "fe002a37-ad6c-4151-b7b3-3d39d133a5f2",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "Dataset({\n",
          "    features: ['title', 'content'],\n",
          "    num_rows: 3\n",
          "})"
         ]
        },
        "execution_count": 24,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "# 选取对应索引的数据，结果还是Dataset\n",
       "datasets[\"train\"].select([0, 20, 100])"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4d90bd91-a594-4af3-b87a-4bfb82d53c97",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "Dataset({\n",
          "    features: ['title', 'content'],\n",
          "    num_rows: 544\n",
          "})"
         ]
        },
        "execution_count": 25,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "# 传个函数做过滤，结果还是Dataset\n",
       "datasets[\"train\"].filter(lambda example: \"中国\" in example[\"title\"])"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "faa28368-b06f-4e84-afbd-ab4581d85bbd",
      "metadata": {},
      "source": [
       "### 数据映射"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 26,
      "id": "782b4cc5-d5ac-4fa5-acca-513644368022",
      "metadata": {},
      "outputs": [],
      "source": [
       "def add_prefix(example):\n",
       "    example[\"title\"] = 'Prefix:' + example[\"title\"]\n",
       "    return example"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 27,
      "id": "a10eae7c",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "['Prefix:望海楼美国打“台湾牌”是危险的赌博',\n",
          " 'Prefix:大力推进高校治理能力建设',\n",
          " 'Prefix:坚持事业为上选贤任能',\n",
          " 'Prefix:“大朋友”的话儿记心头',\n",
          " 'Prefix:用好可持续发展这把“金钥匙”',\n",
          " 'Prefix:跨越雄关，我们走在大路上',\n",
          " 'Prefix:脱贫奇迹彰显政治优势',\n",
          " 'Prefix:拱卫亿万人共同的绿色梦想',\n",
          " 'Prefix:为党育人、为国育才',\n",
          " 'Prefix:净化网络语言']"
         ]
        },
        "execution_count": 27,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "prefix_dataset = datasets.map(add_prefix)\n",
       "prefix_dataset[\"train\"][:10][\"title\"]"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 28,
      "id": "9ceb49f7",
      "metadata": {},
      "outputs": [
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "/node6_1/tanshuai/.conda/envs/abc/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
         "  warnings.warn(\n"
        ]
       }
      ],
      "source": [
       "from transformers import AutoTokenizer\n",
       "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
       "def process_function(example):\n",
       "    model_inputs = tokenizer(example[\"content\"], max_length=512, truncation=True)\n",
       "    labels = tokenizer(example[\"title\"], max_length=32, truncation=True)\n",
       "    # Tokenizer返回字典，其中“inputs_ids”是编码后部分\n",
       "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
       "    return model_inputs"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9de0abdb",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "application/vnd.jupyter.widget-view+json": {
          "model_id": "ef9deb94b6864f41be715640670a10bc",
          "version_major": 2,
          "version_minor": 0
         },
         "text/plain": [
          "Map:   0%|          | 0/1679 [00:00<?, ? examples/s]"
         ]
        },
        "metadata": {},
        "output_type": "display_data"
       },
       {
        "data": {
         "text/plain": [
          "DatasetDict({\n",
          "    train: Dataset({\n",
          "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
          "        num_rows: 5850\n",
          "    })\n",
          "    validation: Dataset({\n",
          "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
          "        num_rows: 1679\n",
          "    })\n",
          "})"
         ]
        },
        "execution_count": 29,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "process_datasets = datasets.map(process_function)\n",
       "process_datasets"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 30,
      "id": "45ee24a9",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "DatasetDict({\n",
          "    train: Dataset({\n",
          "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
          "        num_rows: 5850\n",
          "    })\n",
          "    validation: Dataset({\n",
          "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
          "        num_rows: 1679\n",
          "    })\n",
          "})"
         ]
        },
        "execution_count": 30,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "# 支持批处理\n",
       "sets = datasets.map(process_function, batched=True)\n",
       "process_datasets"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 31,
      "id": "c950b36e",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "DatasetDict({\n",
          "    train: Dataset({\n",
          "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
          "        num_rows: 5850\n",
          "    })\n",
          "    validation: Dataset({\n",
          "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
          "        num_rows: 1679\n",
          "    })\n",
          "})"
         ]
        },
        "execution_count": 31,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "# 多进程处理，指定进程数量\n",
       "process_datasets = datasets.map(process_function, num_proc=8)\n",
       "process_datasets"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 32,
      "id": "145c71f2",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "DatasetDict({\n",
          "    train: Dataset({\n",
          "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
          "        num_rows: 5850\n",
          "    })\n",
          "    validation: Dataset({\n",
          "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
          "        num_rows: 1679\n",
          "    })\n",
          "})"
         ]
        },
        "execution_count": 32,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "# map时把不想要的字段去掉\n",
       "process_datasets = datasets.map(process_function, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
       "process_datasets"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "32f607a1",
      "metadata": {},
      "source": [
       "### 保存与加载"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 33,
      "id": "4bfa1ab0",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "application/vnd.jupyter.widget-view+json": {
          "model_id": "a05c664300944cb6b39b7cfb4dbc22e0",
          "version_major": 2,
          "version_minor": 0
         },
         "text/plain": [
          "Saving the dataset (0/1 shards):   0%|          | 0/5850 [00:00<?, ? examples/s]"
         ]
        },
        "metadata": {},
        "output_type": "display_data"
       },
       {
        "data": {
         "application/vnd.jupyter.widget-view+json": {
          "model_id": "81bab646751d4747aa4995e890915442",
          "version_major": 2,
          "version_minor": 0
         },
         "text/plain": [
          "Saving the dataset (0/1 shards):   0%|          | 0/1679 [00:00<?, ? examples/s]"
         ]
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ],
      "source": [
       "process_datasets.save_to_disk(\"./process_data\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 34,
      "id": "b3ed10d8",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "DatasetDict({\n",
          "    train: Dataset({\n",
          "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
          "        num_rows: 5850\n",
          "    })\n",
          "    validation: Dataset({\n",
          "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
          "        num_rows: 1679\n",
          "    })\n",
          "})"
         ]
        },
        "execution_count": 34,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "process_datasets = load_from_disk(\"./process_data\")\n",
       "process_datasets"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "1ef8ae6a",
      "metadata": {},
      "source": [
       "## 加载本地数据集"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "970d407c",
      "metadata": {},
      "source": [
       "### 直接加载文件作为数据集"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 35,
      "id": "c86e3816",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "application/vnd.jupyter.widget-view+json": {
          "model_id": "cc5202c2989c424eb186689833bfe2f4",
          "version_major": 2,
          "version_minor": 0
         },
         "text/plain": [
          "Generating train split: 0 examples [00:00, ? examples/s]"
         ]
        },
        "metadata": {},
        "output_type": "display_data"
       },
       {
        "data": {
         "text/plain": [
          "DatasetDict({\n",
          "    train: Dataset({\n",
          "        features: ['label', 'review'],\n",
          "        num_rows: 7766\n",
          "    })\n",
          "})"
         ]
        },
        "execution_count": 35,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dataset = load_dataset(\"csv\", data_files=\"./ChnSentiCorp_htl_all.csv\")\n",
       "dataset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 36,
      "id": "36588cf7",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "Dataset({\n",
          "    features: ['label', 'review'],\n",
          "    num_rows: 7766\n",
          "})"
         ]
        },
        "execution_count": 36,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "# 返回类型从DatasetDict变成了Dataset\n",
       "dataset = load_dataset(\"csv\", data_files=\"./ChnSentiCorp_htl_all.csv\", split=\"train\")\n",
       "dataset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 37,
      "id": "03edbed7",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "Dataset({\n",
          "    features: ['label', 'review'],\n",
          "    num_rows: 7766\n",
          "})"
         ]
        },
        "execution_count": 37,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dataset = Dataset.from_csv(\"./ChnSentiCorp_htl_all.csv\", split=\"train\")\n",
       "dataset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 38,
      "id": "5c8f0f26",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "Dataset({\n",
          "    features: ['label', 'review'],\n",
          "    num_rows: 7766\n",
          "})"
         ]
        },
        "execution_count": 38,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dataset = Dataset.from_csv(\"./ChnSentiCorp_htl_all.csv\")\n",
       "dataset"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "0e0c21c0",
      "metadata": {},
      "source": [
       "### 加载文件夹内全部文件作为数据集"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 39,
      "id": "545c2430",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "application/vnd.jupyter.widget-view+json": {
          "model_id": "b6c0df77583746fbb5c325eb69924165",
          "version_major": 2,
          "version_minor": 0
         },
         "text/plain": [
          "Generating train split: 0 examples [00:00, ? examples/s]"
         ]
        },
        "metadata": {},
        "output_type": "display_data"
       },
       {
        "data": {
         "text/plain": [
          "Dataset({\n",
          "    features: ['label', 'review'],\n",
          "    num_rows: 23298\n",
          "})"
         ]
        },
        "execution_count": 39,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dataset = load_dataset(\"csv\", data_dir=\"./all_data\", split='train')\n",
       "dataset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 40,
      "id": "3693c345",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "application/vnd.jupyter.widget-view+json": {
          "model_id": "b563edea06da454499fe29de15ba8bcb",
          "version_major": 2,
          "version_minor": 0
         },
         "text/plain": [
          "Generating train split: 0 examples [00:00, ? examples/s]"
         ]
        },
        "metadata": {},
        "output_type": "display_data"
       },
       {
        "data": {
         "text/plain": [
          "Dataset({\n",
          "    features: ['label', 'review'],\n",
          "    num_rows: 15532\n",
          "})"
         ]
        },
        "execution_count": 40,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dataset = load_dataset(\"csv\", data_files=[\"./all_data//ChnSentiCorp_htl_all.csv\", \"./all_data/ChnSentiCorp_htl_all copy.csv\"], split='train')\n",
       "dataset"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "53c1ebf9",
      "metadata": {},
      "source": [
       "### 通过预先加载的其他格式转换加载数据集"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 41,
      "id": "8212756d",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/html": [
          "<div>\n",
          "<style scoped>\n",
          "    .dataframe tbody tr th:only-of-type {\n",
          "        vertical-align: middle;\n",
          "    }\n",
          "\n",
          "    .dataframe tbody tr th {\n",
          "        vertical-align: top;\n",
          "    }\n",
          "\n",
          "    .dataframe thead th {\n",
          "        text-align: right;\n",
          "    }\n",
          "</style>\n",
          "<table border=\"1\" class=\"dataframe\">\n",
          "  <thead>\n",
          "    <tr style=\"text-align: right;\">\n",
          "      <th></th>\n",
          "      <th>label</th>\n",
          "      <th>review</th>\n",
          "    </tr>\n",
          "  </thead>\n",
          "  <tbody>\n",
          "    <tr>\n",
          "      <th>0</th>\n",
          "      <td>1</td>\n",
          "      <td>距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>1</th>\n",
          "      <td>1</td>\n",
          "      <td>商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>2</th>\n",
          "      <td>1</td>\n",
          "      <td>早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>3</th>\n",
          "      <td>1</td>\n",
          "      <td>宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>4</th>\n",
          "      <td>1</td>\n",
          "      <td>CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风</td>\n",
          "    </tr>\n",
          "  </tbody>\n",
          "</table>\n",
          "</div>"
         ],
         "text/plain": [
          "   label                                             review\n",
          "0      1  距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...\n",
          "1      1                       商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!\n",
          "2      1         早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。\n",
          "3      1  宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小...\n",
          "4      1               CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风"
         ]
        },
        "execution_count": 41,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "import pandas as pd\n",
       "\n",
       "data = pd.read_csv(\"./ChnSentiCorp_htl_all.csv\")\n",
       "data.head()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 42,
      "id": "4803a661",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "Dataset({\n",
          "    features: ['label', 'review'],\n",
          "    num_rows: 7766\n",
          "})"
         ]
        },
        "execution_count": 42,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dataset = Dataset.from_pandas(data)\n",
       "dataset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 47,
      "id": "5a33880e",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "Dataset({\n",
          "    features: ['key', 'text'],\n",
          "    num_rows: 2\n",
          "})"
         ]
        },
        "execution_count": 47,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "# list格式的数据需要内嵌{}，明确数据字段\n",
       "# data = [\"abc\", \"def\", \"ghj\"]\n",
       "data = [{\"key\":\"ghj\", \"text\":\"abc\"}, {\"text\":\"def\"}]\n",
       "Dataset.from_list(data)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 48,
      "id": "77796268",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'key': ['ghj', None], 'text': ['abc', 'def']}"
         ]
        },
        "execution_count": 48,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "Dataset.from_list(data)[:2]"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "40cde6e7",
      "metadata": {},
      "source": [
       "### 通过自定义加载脚本加载数据集"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 54,
      "id": "87f4ae7b",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "application/vnd.jupyter.widget-view+json": {
          "model_id": "c60778fb1da64af78ccf129008acef98",
          "version_major": 2,
          "version_minor": 0
         },
         "text/plain": [
          "Generating train split: 0 examples [00:00, ? examples/s]"
         ]
        },
        "metadata": {},
        "output_type": "display_data"
       },
       {
        "data": {
         "text/plain": [
          "DatasetDict({\n",
          "    train: Dataset({\n",
          "        features: ['paragraphs', 'id', 'title'],\n",
          "        num_rows: 256\n",
          "    })\n",
          "})"
         ]
        },
        "execution_count": 54,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "data = load_dataset(\"json\", data_files=\"./cmrc2018_trial.json\", field=\"data\")\n",
       "data"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 55,
      "id": "b10c3e67",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'paragraphs': [{'context': '基于《跑跑卡丁车》与《泡泡堂》上所开发的游戏，由韩国Nexon开发与发行。中国大陆由盛大游戏运营，这是Nexon时隔6年再次授予盛大网络其游戏运营权。台湾由游戏橘子运营。玩家以水枪、小枪、锤子或是水炸弹泡封敌人(玩家或NPC)，即为一泡封，将水泡击破为一踢爆。若水泡未在时间内踢爆，则会从水泡中释放或被队友救援(即为一救援)。每次泡封会减少生命数，生命数耗完即算为踢爆。重生者在一定时间内为无敌状态，以踢爆数计分较多者获胜，规则因模式而有差异。以2V2、4V4随机配对的方式，玩家可依胜场数爬牌位(依序为原石、铜牌、银牌、金牌、白金、钻石、大师) ，可选择经典、热血、狙击等模式进行游戏。若游戏中离，则4分钟内不得进行配对(每次中离+4分钟)。开放时间为暑假或寒假期间内不定期开放，8人经典模式随机配对，采计分方式，活动时间内分数越多，终了时可依该名次获得奖励。',\n",
          "   'id': 'TRIAL_800',\n",
          "   'qas': [{'answers': [{'answer_start': 127, 'text': '踢爆'}],\n",
          "     'id': 'TRIAL_800_QUERY_0',\n",
          "     'question': '生命数耗完即算为什么？'},\n",
          "    {'answers': [{'answer_start': 301, 'text': '4分钟'}],\n",
          "     'id': 'TRIAL_800_QUERY_1',\n",
          "     'question': '若游戏中离，则多少分钟内不得进行配对？'},\n",
          "    {'answers': [{'answer_start': 85, 'text': '玩家以水枪、小枪、锤子或是水炸弹泡封敌人'}],\n",
          "     'id': 'TRIAL_800_QUERY_2',\n",
          "     'question': '玩家用什么泡封敌人？'},\n",
          "    {'answers': [{'answer_start': 275, 'text': '可选择经典、热血、狙击等模式进行游戏。'}],\n",
          "     'id': 'TRIAL_800_QUERY_3',\n",
          "     'question': '游戏的模式有哪些？'}]}],\n",
          " 'id': 'TRIAL_800',\n",
          " 'title': '泡泡战士'}"
         ]
        },
        "execution_count": 55,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "data[\"train\"][0]"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 57,
      "id": "e7ecab9b",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "application/vnd.jupyter.widget-view+json": {
          "model_id": "a18bd8c4b21c44e0aca33e6ac7587dcd",
          "version_major": 2,
          "version_minor": 0
         },
         "text/plain": [
          "Generating train split: 0 examples [00:00, ? examples/s]"
         ]
        },
        "metadata": {},
        "output_type": "display_data"
       },
       {
        "data": {
         "text/plain": [
          "Dataset({\n",
          "    features: ['id', 'context', 'question', 'answers'],\n",
          "    num_rows: 1002\n",
          "})"
         ]
        },
        "execution_count": 57,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dataset = load_dataset(\"./load_script.py\", split=\"train\", trust_remote_code=True)\n",
       "dataset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 58,
      "id": "56492279",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'id': 'TRIAL_800_QUERY_0',\n",
          " 'context': '基于《跑跑卡丁车》与《泡泡堂》上所开发的游戏，由韩国Nexon开发与发行。中国大陆由盛大游戏运营，这是Nexon时隔6年再次授予盛大网络其游戏运营权。台湾由游戏橘子运营。玩家以水枪、小枪、锤子或是水炸弹泡封敌人(玩家或NPC)，即为一泡封，将水泡击破为一踢爆。若水泡未在时间内踢爆，则会从水泡中释放或被队友救援(即为一救援)。每次泡封会减少生命数，生命数耗完即算为踢爆。重生者在一定时间内为无敌状态，以踢爆数计分较多者获胜，规则因模式而有差异。以2V2、4V4随机配对的方式，玩家可依胜场数爬牌位(依序为原石、铜牌、银牌、金牌、白金、钻石、大师) ，可选择经典、热血、狙击等模式进行游戏。若游戏中离，则4分钟内不得进行配对(每次中离+4分钟)。开放时间为暑假或寒假期间内不定期开放，8人经典模式随机配对，采计分方式，活动时间内分数越多，终了时可依该名次获得奖励。',\n",
          " 'question': '生命数耗完即算为什么？',\n",
          " 'answers': {'text': ['踢爆'], 'answer_start': [127]}}"
         ]
        },
        "execution_count": 58,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dataset[0]"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "c302c965",
      "metadata": {},
      "source": [
       "## Dataset with DataCollator\n",
       "将features特征数据转换为tensor类型的dataset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 59,
      "id": "af8cd0e9",
      "metadata": {},
      "outputs": [],
      "source": [
       "from transformers import DataCollatorWithPadding"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 60,
      "id": "9e7884d6",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "Dataset({\n",
          "    features: ['label', 'review'],\n",
          "    num_rows: 7766\n",
          "})"
         ]
        },
        "execution_count": 60,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dataset = load_dataset(\"csv\", data_files=\"./ChnSentiCorp_htl_all.csv\", split=\"train\")\n",
       "dataset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 62,
      "id": "97d493fe",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "application/vnd.jupyter.widget-view+json": {
          "model_id": "0f2bc149f95340feb1e4d6b16ddae9ec",
          "version_major": 2,
          "version_minor": 0
         },
         "text/plain": [
          "Filter:   0%|          | 0/7765 [00:00<?, ? examples/s]"
         ]
        },
        "metadata": {},
        "output_type": "display_data"
       },
       {
        "data": {
         "text/plain": [
          "Dataset({\n",
          "    features: ['label', 'review'],\n",
          "    num_rows: 7765\n",
          "})"
         ]
        },
        "execution_count": 62,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dataset = dataset.filter(lambda x: x[\"review\"] is not None)\n",
       "dataset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 63,
      "id": "16e9aff1",
      "metadata": {},
      "outputs": [],
      "source": [
       "def process_function(examples):\n",
       "    tokenized_examples = tokenizer(examples[\"review\"], max_length=128, truncation=True)\n",
       "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
       "    return tokenized_examples"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 64,
      "id": "e2a62bf2",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "application/vnd.jupyter.widget-view+json": {
          "model_id": "b362c5af820b44ae95eb685d0e409cf6",
          "version_major": 2,
          "version_minor": 0
         },
         "text/plain": [
          "Map:   0%|          | 0/7765 [00:00<?, ? examples/s]"
         ]
        },
        "metadata": {},
        "output_type": "display_data"
       },
       {
        "data": {
         "text/plain": [
          "Dataset({\n",
          "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
          "    num_rows: 7765\n",
          "})"
         ]
        },
        "execution_count": 64,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "tokenized_dataset = dataset.map(process_function, batched=True, remove_columns=dataset.column_names)\n",
       "tokenized_dataset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 68,
      "id": "99ea0160",
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "{'input_ids': [[101, 6655, 4895, 2335, 3763, 1062, 6662, 6772, 6818, 117, 852, 3221, 1062, 769, 2900, 4850, 679, 2190, 117, 1963, 3362, 3221, 107, 5918, 7355, 5296, 107, 4638, 6413, 117, 833, 7478, 2382, 7937, 4172, 119, 2456, 6379, 4500, 1166, 4638, 6662, 5296, 119, 2791, 7313, 6772, 711, 5042, 1296, 119, 102], [101, 1555, 1218, 1920, 2414, 2791, 8024, 2791, 7313, 2523, 1920, 8024, 2414, 3300, 100, 2160, 8024, 3146, 860, 2697, 6230, 5307, 3845, 2141, 2669, 679, 7231, 106, 102], [101, 3193, 7623, 1922, 2345, 8024, 3187, 6389, 1343, 1914, 2208, 782, 8024, 6929, 6804, 738, 679, 1217, 7608, 1501, 4638, 511, 6983, 2421, 2418, 6421, 7028, 6228, 671, 678, 6821, 702, 7309, 7579, 749, 511, 2791, 7313, 3315, 6716, 2523, 1962, 511, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [1, 1, 1]}\n"
        ]
       }
      ],
      "source": [
       "print(tokenized_dataset[:3])"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 69,
      "id": "cf828f49",
      "metadata": {},
      "outputs": [],
      "source": [
       "collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 70,
      "id": "6dc0fb46",
      "metadata": {},
      "outputs": [],
      "source": [
       "from torch.utils.data import DataLoader"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 71,
      "id": "5e8be670",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "<torch.utils.data.dataloader.DataLoader at 0x7fc1a7af2640>"
         ]
        },
        "execution_count": 71,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dl = DataLoader(tokenized_dataset, batch_size=4, collate_fn=collator, shuffle=True)\n",
       "dl"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 72,
      "id": "05c3f6ad",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "(0,\n",
          " {'input_ids': tensor([[ 101, 2769, 3315, 3341, 4157, 6397, 3221,  671, 1146, 4638, 8024,  679,\n",
          "          4761, 2582,  720, 1359, 2768,  749,  123, 1146, 8024, 2792,  809, 1086,\n",
          "          3341, 3121,  671,  678,  511,  102,    0,    0,    0,    0,    0,    0,\n",
          "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
          "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
          "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
          "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
          "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
          "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
          "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
          "             0,    0,    0,    0,    0,    0,    0,    0],\n",
          "         [ 101, 6821,  702, 6983, 2421, 3221, 2769,  857, 6814, 4638, 1744, 1079,\n",
          "          1724, 3215, 5277, 6983, 2421, 7027, 3297, 2345, 4638,  671, 7313, 8024,\n",
          "          3302, 1218, 2578, 2428, 1469, 6574, 7030, 6963, 2523, 2345, 8024, 2791,\n",
          "          7313, 2523, 2207, 8024, 3193, 7623, 6825, 4157, 5489, 6963, 3766, 3300,\n",
          "          8024,  130, 8038, 8136, 5310, 3338, 8024,  129, 8038, 8136, 1168, 4638,\n",
          "          8024, 1825, 3315, 2218, 3766, 3300, 1391, 4638,  749, 8024, 5445,  684,\n",
          "          3302, 1218, 1447,  738,  679, 5314, 1217, 8013,  738, 6387, 3221, 1728,\n",
          "           711, 3180, 3952, 3250, 4157, 4638, 7309, 7579, 8024,  852, 3221, 2769,\n",
          "           812,  857, 6814, 7353, 6818, 1071,  800, 4638,  676, 3215, 5277, 6983,\n",
          "          2421, 8024, 1392, 3175, 7481, 6963, 3683, 6821, 2157, 1962, 8024,  817,\n",
          "          3419, 6820, 5543,  912, 2139, 8135, 1914,  102],\n",
          "         [ 101, 1184, 1378, 3302, 1218, 2523,  679, 1962, 8024,  855, 5390, 6820,\n",
          "          5050, 1377,  809, 8024, 2791, 7313, 2697, 6230, 3766, 1962, 2397, 1112,\n",
          "          8024, 3193, 7623, 1501, 4905, 1922, 2208,  511, 1506, 1506, 8024, 1377,\n",
          "          6586, 4638, 3221, 8024, 6820, 4500, 4638, 3221, 7603, 3382, 1469, 5653,\n",
          "          3302,  881,  511,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
          "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
          "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
          "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
          "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
          "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
          "             0,    0,    0,    0,    0,    0,    0,    0],\n",
          "         [ 101, 6983, 2421, 1184, 1378, 3302, 1218, 2523, 2345, 8013, 2458,  749,\n",
          "          2791, 7313, 8024, 6822, 1343,  671, 4692, 8024, 4994, 4197, 3300,  782,\n",
          "          1057,  857,  749, 8013, 6820, 1962, 3766, 1139,  784,  720,  752, 8013,\n",
          "          6206, 1139,  784,  720,  752, 6929, 6443, 6566, 6569, 1450, 8043, 2823,\n",
          "          1184, 1378, 7566, 4408, 1353, 3216, 8024, 7566, 4408,  671, 1359, 7733,\n",
          "          3302, 1218, 1447,  671, 6804, 5314, 2769, 2940, 2791, 7313, 8013, 5567,\n",
          "           677, 6825, 5010, 2159,  738, 3766, 3300, 8013, 1962, 6496, 3221, 2769,\n",
          "           812, 7560, 2145, 4306, 7231, 8013,  679, 4761, 6887, 3018,  784,  720,\n",
          "          7787, 8013, 5314, 3025, 1814, 2802, 4510, 6413, 2832, 6401, 8024, 3025,\n",
          "          1814, 4638, 2578, 2428, 2523, 1962, 8013, 6983, 2421, 5307, 4415, 2802,\n",
          "          1168, 2791, 7313, 3341, 6608, 4851, 6887,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0],\n",
          "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0],\n",
          "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0],\n",
          "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
          "          1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0],\n",
          "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
          "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
          "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
          "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
          "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
          "          1, 1, 1, 1, 1, 1, 1, 1],\n",
          "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
          "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
          "          1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
          "          0, 0, 0, 0, 0, 0, 0, 0],\n",
          "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
          "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
          "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
          "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
          "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
          "          1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([0, 0, 1, 0])})"
         ]
        },
        "execution_count": 72,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "next(enumerate(dl))"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 74,
      "id": "3553fe2d",
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "torch.Size([4, 128])\n",
         "torch.Size([4, 128])\n",
         "torch.Size([4, 128])\n",
         "torch.Size([4, 107])\n",
         "torch.Size([4, 128])\n",
         "torch.Size([4, 128])\n",
         "torch.Size([4, 128])\n",
         "torch.Size([4, 80])\n",
         "torch.Size([4, 128])\n",
         "torch.Size([4, 128])\n",
         "torch.Size([4, 73])\n"
        ]
       }
      ],
      "source": [
       "num = 0\n",
       "for batch in dl:\n",
       "    print(batch[\"input_ids\"].size())\n",
       "    num += 1\n",
       "    if num > 10:\n",
       "        break"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   
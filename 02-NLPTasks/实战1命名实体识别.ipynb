{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 命名实体识别(Named Entity Recognition, NER)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Step1 导入相关包"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
       "import evaluate\n",
       "from datasets import load_dataset\n",
       "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Step2 加载数据集"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "DatasetDict({\n",
          "    train: Dataset({\n",
          "        features: ['id', 'tokens', 'ner_tags'],\n",
          "        num_rows: 20865\n",
          "    })\n",
          "    validation: Dataset({\n",
          "        features: ['id', 'tokens', 'ner_tags'],\n",
          "        num_rows: 2319\n",
          "    })\n",
          "    test: Dataset({\n",
          "        features: ['id', 'tokens', 'ner_tags'],\n",
          "        num_rows: 4637\n",
          "    })\n",
          "})"
         ]
        },
        "execution_count": 2,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "from datasets import DatasetDict\n",
       "ner_datasets = DatasetDict.load_from_disk(\"ner_data\")\n",
       "ner_datasets"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'id': '0',\n",
          " 'tokens': ['海',\n",
          "  '钓',\n",
          "  '比',\n",
          "  '赛',\n",
          "  '地',\n",
          "  '点',\n",
          "  '在',\n",
          "  '厦',\n",
          "  '门',\n",
          "  '与',\n",
          "  '金',\n",
          "  '门',\n",
          "  '之',\n",
          "  '间',\n",
          "  '的',\n",
          "  '海',\n",
          "  '域',\n",
          "  '。'],\n",
          " 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0]}"
         ]
        },
        "execution_count": 3,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "ner_datasets[\"train\"][0]"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'id': Value(dtype='string', id=None),\n",
          " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
          " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)}"
         ]
        },
        "execution_count": 4,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "ner_datasets[\"train\"].features"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"
         ]
        },
        "execution_count": 5,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "label_list = ner_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
       "label_list"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Step3 数据预处理"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
       "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'input_ids': [101, 3862, 7157, 3683, 6612, 1765, 4157, 1762, 1336, 7305, 680, 7032, 7305, 722, 7313, 4638, 3862, 1818, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
         ]
        },
        "execution_count": 7,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "tokenizer(ner_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)  # 将每个字符串看成字而不是句子，避免出现[CLS][SEP]等"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'input_ids': [101, 10673, 12865, 12921, 8181, 162, 10716, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
         ]
        },
        "execution_count": 8,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "res = tokenizer(\"interesting thing\")\n",
       "res"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "[None, 0, 0, 0, 0, 1, 1, None]"
         ]
        },
        "execution_count": 9,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "res.word_ids()  # token属于哪一个word"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
       "def process_function(examples):\n",
       "    tokenized_examples = tokenizer(examples[\"tokens\"], max_length=128, truncation=True, is_split_into_words=True)\n",
       "    labels = []\n",
       "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
       "        word_ids = tokenized_examples.word_ids(batch_index=i)\n",
       "        label_ids = []\n",
       "        for word_id in word_ids:\n",
       "            if word_id is None:\n",
       "                label_ids.append(-100)\n",
       "            else:\n",
       "                label_ids.append(label[word_id])\n",
       "        labels.append(label_ids)\n",
       "    tokenized_examples[\"labels\"] = labels\n",
       "    return tokenized_examples"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "application/vnd.jupyter.widget-view+json": {
          "model_id": "9597abac0c6e4bd6a0021b6867d5a1c6",
          "version_major": 2,
          "version_minor": 0
         },
         "text/plain": [
          "Map:   0%|          | 0/2319 [00:00<?, ? examples/s]"
         ]
        },
        "metadata": {},
        "output_type": "display_data"
       },
       {
        "data": {
         "text/plain": [
          "DatasetDict({\n",
          "    train: Dataset({\n",
          "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
          "        num_rows: 20865\n",
          "    })\n",
          "    validation: Dataset({\n",
          "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
          "        num_rows: 2319\n",
          "    })\n",
          "    test: Dataset({\n",
          "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
          "        num_rows: 4637\n",
          "    })\n",
          "})"
         ]
        },
        "execution_count": 11,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "tokenized_datasets = ner_datasets.map(process_function, batched=True)\n",
       "tokenized_datasets"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'id': '0',\n",
          " 'tokens': ['海',\n",
          "  '钓',\n",
          "  '比',\n",
          "  '赛',\n",
          "  '地',\n",
          "  '点',\n",
          "  '在',\n",
          "  '厦',\n",
          "  '门',\n",
          "  '与',\n",
          "  '金',\n",
          "  '门',\n",
          "  '之',\n",
          "  '间',\n",
          "  '的',\n",
          "  '海',\n",
          "  '域',\n",
          "  '。'],\n",
          " 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0],\n",
          " 'input_ids': [101,\n",
          "  3862,\n",
          "  7157,\n",
          "  3683,\n",
          "  6612,\n",
          "  1765,\n",
          "  4157,\n",
          "  1762,\n",
          "  1336,\n",
          "  7305,\n",
          "  680,\n",
          "  7032,\n",
          "  7305,\n",
          "  722,\n",
          "  7313,\n",
          "  4638,\n",
          "  3862,\n",
          "  1818,\n",
          "  511,\n",
          "  102],\n",
          " 'token_type_ids': [0,\n",
          "  0,\n",
          "  0,\n",
          "  0,\n",
          "  0,\n",
          "  0,\n",
          "  0,\n",
          "  0,\n",
          "  0,\n",
          "  0,\n",
          "  0,\n",
          "  0,\n",
          "  0,\n",
          "  0,\n",
          "  0,\n",
          "  0,\n",
          "  0,\n",
          "  0,\n",
          "  0,\n",
          "  0],\n",
          " 'attention_mask': [1,\n",
          "  1,\n",
          "  1,\n",
          "  1,\n",
          "  1,\n",
          "  1,\n",
          "  1,\n",
          "  1,\n",
          "  1,\n",
          "  1,\n",
          "  1,\n",
          "  1,\n",
          "  1,\n",
          "  1,\n",
          "  1,\n",
          "  1,\n",
          "  1,\n",
          "  1,\n",
          "  1,\n",
          "  1],\n",
          " 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0, -100]}"
         ]
        },
        "execution_count": 12,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "tokenized_datasets[\"train\"][0]"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Step4 创建模型"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "Some weights of BertForTokenClassification were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
         "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        ]
       }
      ],
      "source": [
       "# 对于所有的非二分类任务，切记要指定num_labels，否则就会device错误\n",
       "model = AutoModelForTokenClassification.from_pretrained(\"hfl/chinese-macbert-base\", num_labels=len(label_list))"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Step5 创建评估函数"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "EvaluationModule(name: \"seqeval\", module_type: \"metric\", features: {'predictions': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence')}, usage: \"\"\"\n",
          "Produces labelling scores along with its sufficient statistics\n",
          "from a source against one or more references.\n",
          "\n",
          "Args:\n",
          "    predictions: List of List of predicted labels (Estimated targets as returned by a tagger)\n",
          "    references: List of List of reference labels (Ground truth (correct) target values)\n",
          "    suffix: True if the IOB prefix is after type, False otherwise. default: False\n",
          "    scheme: Specify target tagging scheme. Should be one of [\"IOB1\", \"IOB2\", \"IOE1\", \"IOE2\", \"IOBES\", \"BILOU\"].\n",
          "        default: None\n",
          "    mode: Whether to count correct entity labels with incorrect I/B tags as true positives or not.\n",
          "        If you want to only count exact matches, pass mode=\"strict\". default: None.\n",
          "    sample_weight: Array-like of shape (n_samples,), weights for individual samples. default: None\n",
          "    zero_division: Which value to substitute as a metric value when encountering zero division. Should be on of 0, 1,\n",
          "        \"warn\". \"warn\" acts as 0, but the warning is raised.\n",
          "\n",
          "Returns:\n",
          "    'scores': dict. Summary of the scores for overall and per type\n",
          "        Overall:\n",
          "            'accuracy': accuracy,\n",
          "            'precision': precision,\n",
          "            'recall': recall,\n",
          "            'f1': F1 score, also known as balanced F-score or F-measure,\n",
          "        Per type:\n",
          "            'precision': precision,\n",
          "            'recall': recall,\n",
          "            'f1': F1 score, also known as balanced F-score or F-measure\n",
          "Examples:\n",
          "\n",
          "    >>> predictions = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
          "    >>> references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
          "    >>> seqeval = evaluate.load(\"seqeval\")\n",
          "    >>> results = seqeval.compute(predictions=predictions, references=references)\n",
          "    >>> print(list(results.keys()))\n",
          "    ['MISC', 'PER', 'overall_precision', 'overall_recall', 'overall_f1', 'overall_accuracy']\n",
          "    >>> print(results[\"overall_f1\"])\n",
          "    0.5\n",
          "    >>> print(results[\"PER\"][\"f1\"])\n",
          "    1.0\n",
          "\"\"\", stored examples: 0)"
         ]
        },
        "execution_count": 14,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "seqeval = evaluate.load(\"seqeval\")\n",
       "seqeval"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
       "import numpy as np\n",
       "\n",
       "def eval_metric(pred):\n",
       "    predictions, labels = pred\n",
       "    predictions = np.argmax(predictions,axis=-1)\n",
       "\n",
       "    true_predictions = [\n",
       "        [label_list[p] for p, l in zip(prediction, label) if l != -100]\n",
       "        for prediction, label in zip(predictions, labels)\n",
       "    ]\n",
       "\n",
       "    true_labels = [\n",
       "        [label_list[l] for p, l in zip(prediction, label) if l != -100]\n",
       "        for prediction, label in zip(predictions, labels)\n",
       "    ]\n",
       "\n",
       "    result = seqeval.compute(predictions=true_predictions, \n",
       "                    references=true_labels,\n",
       "                    mode=\"strict\",\n",
       "                    scheme=\"IOB2\")\n",
       "    \n",
       "    return {\n",
       "        \"f1\": result[\"overall_f1\"]\n",
       "    }"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Step6 配置训练参数"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "/node6_1/tanshuai/.conda/envs/abc/lib/python3.9/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
         "  warnings.warn(\n",
         "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
         "To disable this warning, you can either:\n",
         "\t- Avoid using `tokenizers` before the fork if possible\n",
         "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
         "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
         "To disable this warning, you can either:\n",
         "\t- Avoid using `tokenizers` before the fork if possible\n",
         "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
         "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
         "To disable this warning, you can either:\n",
         "\t- Avoid using `tokenizers` before the fork if possible\n",
         "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
        ]
       }
      ],
      "source": [
       "args = TrainingArguments(\n",
       "    output_dir=\"models for ner\",\n",
       "    per_device_train_batch_size=64,\n",
       "    per_device_eval_batch_size=128,\n",
       "    evaluation_strategy=\"epoch\",\n",
       "    save_strategy=\"epoch\",\n",
       "    metric_for_best_model=\"f1\",\n",
       "    load_best_model_at_end=True,\n",
       "    logging_steps=50\n",
       ")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Step7 创建Trainer"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
       "trainer = Trainer(\n",
       "    model=model,\n",
       "    args=args,\n",
       "    tokenizer=tokenizer,\n",
       "    train_dataset=tokenized_datasets[\"train\"],\n",
       "    eval_dataset=tokenized_datasets[\"validation\"],\n",
       "    compute_metrics=eval_metric,\n",
       "    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
       ")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Step8 模型训练"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "/node6_1/tanshuai/.conda/envs/abc/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
         "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        ]
       },
       {
        "data": {
         "text/html": [
          "\n",
          "    <div>\n",
          "      \n",
          "      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
          "      [123/123 02:11, Epoch 3/3]\n",
          "    </div>\n",
          "    <table border=\"1\" class=\"dataframe\">\n",
          "  <thead>\n",
          " <tr style=\"text-align: left;\">\n",
          "      <th>Epoch</th>\n",
          "      <th>Training Loss</th>\n",
          "      <th>Validation Loss</th>\n",
          "      <th>F1</th>\n",
          "    </tr>\n",
          "  </thead>\n",
          "  <tbody>\n",
          "    <tr>\n",
          "      <td>1</td>\n",
          "      <td>No log</td>\n",
          "      <td>0.035783</td>\n",
          "      <td>0.887185</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <td>2</td>\n",
          "      <td>0.188100</td>\n",
          "      <td>0.024953</td>\n",
          "      <td>0.919486</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <td>3</td>\n",
          "      <td>0.029600</td>\n",
          "      <td>0.022787</td>\n",
          "      <td>0.927708</td>\n",
          "    </tr>\n",
          "  </tbody>\n",
          "</table><p>"
         ],
         "text/plain": [
          "<IPython.core.display.HTML object>"
         ]
        },
        "metadata": {},
        "output_type": "display_data"
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "/node6_1/tanshuai/.conda/envs/abc/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
         "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
         "/node6_1/tanshuai/.conda/envs/abc/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
         "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
         "/node6_1/tanshuai/.conda/envs/abc/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
         "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        ]
       },
       {
        "data": {
         "text/plain": [
          "TrainOutput(global_step=123, training_loss=0.0924070305455991, metrics={'train_runtime': 147.3654, 'train_samples_per_second': 424.761, 'train_steps_per_second': 0.835, 'total_flos': 4089152462503680.0, 'train_loss': 0.0924070305455991, 'epoch': 3.0})"
         ]
        },
        "execution_count": 18,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "trainer.train()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "/node6_1/tanshuai/.conda/envs/abc/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
         "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
        ]
       },
       {
        "data": {
         "text/html": [
          "\n",
          "    <div>\n",
          "      \n",
          "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
          "      [5/5 00:02]\n",
          "    </div>\n",
          "    "
         ],
         "text/plain": [
          "<IPython.core.display.HTML object>"
         ]
        },
        "metadata": {},
        "output_type": "display_data"
       },
       {
        "data": {
         "text/plain": [
          "{'eval_loss': 0.028748637065291405,\n",
          " 'eval_f1': 0.9222214778589134,\n",
          " 'eval_runtime': 6.1719,\n",
          " 'eval_samples_per_second': 751.303,\n",
          " 'eval_steps_per_second': 0.81,\n",
          " 'epoch': 3.0}"
         ]
        },
        "execution_count": 19,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "trainer.evaluate(tokenized_datasets[\"test\"])"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Step9 模型预测"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
       "from transformers import pipeline"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "BertConfig {\n",
         "  \"_name_or_path\": \"hfl/chinese-macbert-base\",\n",
         "  \"architectures\": [\n",
         "    \"BertForTokenClassification\"\n",
         "  ],\n",
         "  \"attention_probs_dropout_prob\": 0.1,\n",
         "  \"classifier_dropout\": null,\n",
         "  \"directionality\": \"bidi\",\n",
         "  \"gradient_checkpointing\": false,\n",
         "  \"hidden_act\": \"gelu\",\n",
         "  \"hidden_dropout_prob\": 0.1,\n",
         "  \"hidden_size\": 768,\n",
         "  \"id2label\": {\n",
         "    \"0\": \"O\",\n",
         "    \"1\": \"B-PER\",\n",
         "    \"2\": \"I-PER\",\n",
         "    \"3\": \"B-ORG\",\n",
         "    \"4\": \"I-ORG\",\n",
         "    \"5\": \"B-LOC\",\n",
         "    \"6\": \"I-LOC\"\n",
         "  },\n",
         "  \"initializer_range\": 0.02,\n",
         "  \"intermediate_size\": 3072,\n",
         "  \"label2id\": {\n",
         "    \"LABEL_0\": 0,\n",
         "    \"LABEL_1\": 1,\n",
         "    \"LABEL_2\": 2,\n",
         "    \"LABEL_3\": 3,\n",
         "    \"LABEL_4\": 4,\n",
         "    \"LABEL_5\": 5,\n",
         "    \"LABEL_6\": 6\n",
         "  },\n",
         "  \"layer_norm_eps\": 1e-12,\n",
         "  \"max_position_embeddings\": 512,\n",
         "  \"model_type\": \"bert\",\n",
         "  \"num_attention_heads\": 12,\n",
         "  \"num_hidden_layers\": 12,\n",
         "  \"pad_token_id\": 0,\n",
         "  \"pooler_fc_size\": 768,\n",
         "  \"pooler_num_attention_heads\": 12,\n",
         "  \"pooler_num_fc_layers\": 3,\n",
         "  \"pooler_size_per_head\": 128,\n",
         "  \"pooler_type\": \"first_token_transform\",\n",
         "  \"position_embedding_type\": \"absolute\",\n",
         "  \"torch_dtype\": \"float32\",\n",
         "  \"transformers_version\": \"4.42.4\",\n",
         "  \"type_vocab_size\": 2,\n",
         "  \"use_cache\": true,\n",
         "  \"vocab_size\": 21128\n",
         "}\n",
         "\n",
         "BertConfig {\n",
         "  \"_name_or_path\": \"hfl/chinese-macbert-base\",\n",
         "  \"architectures\": [\n",
         "    \"BertForTokenClassification\"\n",
         "  ],\n",
         "  \"attention_probs_dropout_prob\": 0.1,\n",
         "  \"classifier_dropout\": null,\n",
         "  \"directionality\": \"bidi\",\n",
         "  \"gradient_checkpointing\": false,\n",
         "  \"hidden_act\": \"gelu\",\n",
         "  \"hidden_dropout_prob\": 0.1,\n",
         "  \"hidden_size\": 768,\n",
         "  \"id2label\": {\n",
         "    \"0\": \"O\",\n",
         "    \"1\": \"B-PER\",\n",
         "    \"2\": \"I-PER\",\n",
         "    \"3\": \"B-ORG\",\n",
         "    \"4\": \"I-ORG\",\n",
         "    \"5\": \"B-LOC\",\n",
         "    \"6\": \"I-LOC\"\n",
         "  },\n",
         "  \"initializer_range\": 0.02,\n",
         "  \"intermediate_size\": 3072,\n",
         "  \"label2id\": {\n",
         "    \"LABEL_0\": 0,\n",
         "    \"LABEL_1\": 1,\n",
         "    \"LABEL_2\": 2,\n",
         "    \"LABEL_3\": 3,\n",
         "    \"LABEL_4\": 4,\n",
         "    \"LABEL_5\": 5,\n",
         "    \"LABEL_6\": 6\n",
         "  },\n",
         "  \"layer_norm_eps\": 1e-12,\n",
         "  \"max_position_embeddings\": 512,\n",
         "  \"model_type\": \"bert\",\n",
         "  \"num_attention_heads\": 12,\n",
         "  \"num_hidden_layers\": 12,\n",
         "  \"pad_token_id\": 0,\n",
         "  \"pooler_fc_size\": 768,\n",
         "  \"pooler_num_attention_heads\": 12,\n",
         "  \"pooler_num_fc_layers\": 3,\n",
         "  \"pooler_size_per_head\": 128,\n",
         "  \"pooler_type\": \"first_token_transform\",\n",
         "  \"position_embedding_type\": \"absolute\",\n",
         "  \"torch_dtype\": \"float32\",\n",
         "  \"transformers_version\": \"4.42.4\",\n",
         "  \"type_vocab_size\": 2,\n",
         "  \"use_cache\": true,\n",
         "  \"vocab_size\": 21128\n",
         "}\n",
         "\n"
        ]
       }
      ],
      "source": [
       "# 使用pipeline进行推理，要指定id2label\n",
       "print(model.config)\n",
       "model.config.id2label = {idx: label for idx, label in enumerate(label_list)}\n",
       "print(model.config)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
        ]
       }
      ],
      "source": [
       "# 对于NER任务，可以指定aggregation_strategy为simple，得到具体的实体的结果，而不是token的结果\n",
       "ner_pipe = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, device_map=True, aggregation_strategy=\"simple\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "[{'entity_group': 'PER',\n",
          "  'score': 0.3923845,\n",
          "  'word': '明',\n",
          "  'start': 1,\n",
          "  'end': 2},\n",
          " {'entity_group': 'LOC',\n",
          "  'score': 0.9914744,\n",
          "  'word': '北 京',\n",
          "  'start': 3,\n",
          "  'end': 5}]"
         ]
        },
        "execution_count": 34,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "res = ner_pipe(\"小明在北京上班\")\n",
       "res"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "'小 明'"
         ]
        },
        "execution_count": 33,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "tokenizer.decode([2207, 3209])"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'PER': ['明'], 'LOC': ['北京']}"
         ]
        },
        "execution_count": 35,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "# 根据start和end取实际的结果\n",
       "ner_result = {}\n",
       "x = \"小明在北京上班\"\n",
       "for r in res:\n",
       "    if r[\"entity_group\"] not in ner_result:\n",
       "        ner_result[r[\"entity_group\"]] = []\n",
       "    ner_result[r[\"entity_group\"]].append(x[r[\"start\"]: r[\"end\"]])\n",
       "\n",
       "ner_result"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 2
   }
   
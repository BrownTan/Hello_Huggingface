{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT 进阶操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、自定义模型适配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from peft import LoraConfig, get_peft_model, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1 = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 2)\n",
    ")\n",
    "net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight\n",
      "0.bias\n",
      "2.weight\n",
      "2.bias\n"
     ]
    }
   ],
   "source": [
    "for name, para in net1.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = LoraConfig(target_modules=[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=10, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=10, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=10, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = get_peft_model(net1, config1)\n",
    "model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、多适配器加载与切换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save_pretrained(\"lora1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False, r=8, target_modules={'2'}, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config2 = LoraConfig(target_modules=[\"2\"])\n",
    "config2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): lora.Linear(\n",
       "    (base_layer): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (lora_dropout): ModuleDict(\n",
       "      (default): Identity()\n",
       "    )\n",
       "    (lora_A): ModuleDict(\n",
       "      (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "    )\n",
       "    (lora_B): ModuleDict(\n",
       "      (default): Linear(in_features=8, out_features=10, bias=False)\n",
       "    )\n",
       "    (lora_embedding_A): ParameterDict()\n",
       "    (lora_embedding_B): ParameterDict()\n",
       "    (lora_magnitude_vector): ModuleDict()\n",
       "  )\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=10, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=10, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=2, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=2, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = get_peft_model(net1, config2)\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save_pretrained(\"lora2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2 = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 2)\n",
    ")\n",
    "net2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=10, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (lora1): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (lora1): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (lora1): Linear(in_features=8, out_features=10, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=10, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(net2, model_id=\"lora1\", adapter_name=\"lora1\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=10, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (lora1): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (lora1): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (lora1): Linear(in_features=8, out_features=10, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=2, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (lora2): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (lora2): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (lora2): Linear(in_features=8, out_features=2, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_adapter(\"lora2\", adapter_name=\"lora2\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lora1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.active_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9952,  1.1457]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.arange(0, 10).view(1, 10).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.0.base_layer.weight Parameter containing:\n",
      "tensor([[-0.0414, -0.1651,  0.2459, -0.1529,  0.2911, -0.0136,  0.2096, -0.0513,\n",
      "          0.0600, -0.0765],\n",
      "        [ 0.1274,  0.2460,  0.2658,  0.2475,  0.1282,  0.1452, -0.3047,  0.0442,\n",
      "          0.2055,  0.0526],\n",
      "        [ 0.1313, -0.2728,  0.2724, -0.1882,  0.2271,  0.3057,  0.2635, -0.2568,\n",
      "         -0.2739,  0.0197],\n",
      "        [ 0.3041,  0.2124, -0.1669, -0.1543, -0.2298, -0.0651, -0.1738,  0.0727,\n",
      "         -0.1295,  0.2249],\n",
      "        [-0.0296,  0.0120,  0.2147, -0.0565, -0.1278,  0.0752, -0.0741,  0.1854,\n",
      "         -0.1090, -0.1683],\n",
      "        [ 0.2565,  0.2665,  0.2329,  0.2191,  0.0364, -0.0440, -0.1142, -0.1982,\n",
      "         -0.1645,  0.1259],\n",
      "        [ 0.1487, -0.1933, -0.2263,  0.0323,  0.2159,  0.0452, -0.0217,  0.2404,\n",
      "          0.1106,  0.2523],\n",
      "        [ 0.1606,  0.2354, -0.0365, -0.1610,  0.2193, -0.0841, -0.2802, -0.2409,\n",
      "         -0.3083,  0.1488],\n",
      "        [-0.1851,  0.0918,  0.1800,  0.0834, -0.0813, -0.1944,  0.2672, -0.1264,\n",
      "          0.2792, -0.2359],\n",
      "        [ 0.0611, -0.0598,  0.2456, -0.0349,  0.1140, -0.0976,  0.3054,  0.3143,\n",
      "         -0.1980,  0.3039]])\n",
      "base_model.model.0.base_layer.bias Parameter containing:\n",
      "tensor([ 0.2733, -0.3000, -0.1584,  0.1468, -0.0813, -0.1223, -0.1552,  0.1441,\n",
      "         0.2311, -0.2951])\n",
      "base_model.model.0.lora_A.lora1.weight Parameter containing:\n",
      "tensor([[ 0.0078,  0.0148, -0.1759,  0.1745, -0.0623, -0.1550,  0.1817,  0.0496,\n",
      "         -0.3109,  0.1893],\n",
      "        [-0.1714, -0.1696, -0.0238, -0.2245,  0.1016,  0.2010, -0.2888,  0.0153,\n",
      "         -0.1784,  0.0984],\n",
      "        [ 0.0133,  0.0544, -0.2350,  0.0650, -0.2355,  0.0317,  0.2949,  0.0201,\n",
      "         -0.3005,  0.2395],\n",
      "        [ 0.1457, -0.2216,  0.0649, -0.3069, -0.1075,  0.0965, -0.1697, -0.1609,\n",
      "          0.0323, -0.1337],\n",
      "        [ 0.1827,  0.2065,  0.2220,  0.2874,  0.2042,  0.2973, -0.0809,  0.1647,\n",
      "         -0.2466, -0.0409],\n",
      "        [-0.1060,  0.1530, -0.1454, -0.1384, -0.0266,  0.1807, -0.0340, -0.1187,\n",
      "         -0.1356, -0.0733],\n",
      "        [ 0.0346, -0.3116,  0.3099,  0.1621,  0.2623,  0.2239, -0.1802,  0.1662,\n",
      "         -0.1684, -0.1923],\n",
      "        [ 0.1295, -0.1498,  0.0746, -0.2818,  0.3102, -0.1374, -0.2619,  0.1777,\n",
      "          0.1993, -0.2776]], requires_grad=True)\n",
      "base_model.model.0.lora_B.lora1.weight Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True)\n",
      "base_model.model.2.base_layer.weight Parameter containing:\n",
      "tensor([[-0.0996, -0.2845, -0.0490,  0.2142, -0.0459, -0.2946, -0.0692, -0.0457,\n",
      "         -0.3008, -0.0324],\n",
      "        [-0.1326, -0.1971,  0.0062, -0.2094,  0.0141, -0.0783,  0.1845, -0.2843,\n",
      "         -0.2343,  0.2208]])\n",
      "base_model.model.2.base_layer.bias Parameter containing:\n",
      "tensor([-0.2727,  0.0288])\n",
      "base_model.model.2.lora_A.lora2.weight Parameter containing:\n",
      "tensor([[ 0.0560, -0.2291, -0.0342,  0.0644, -0.1457,  0.0006, -0.1726, -0.2194,\n",
      "          0.0572, -0.2965],\n",
      "        [ 0.0951,  0.0689, -0.2202,  0.1598, -0.3139, -0.2686, -0.0715,  0.1660,\n",
      "          0.0465,  0.2941],\n",
      "        [ 0.3139, -0.2672, -0.0280,  0.0099,  0.0837, -0.2127,  0.0765,  0.0465,\n",
      "         -0.0701, -0.0645],\n",
      "        [-0.0817,  0.0983,  0.1342, -0.2732, -0.3015,  0.0461,  0.1978, -0.0057,\n",
      "          0.1041, -0.1917],\n",
      "        [-0.1363, -0.1127, -0.1446,  0.0575,  0.0802, -0.1865,  0.0389, -0.2113,\n",
      "          0.2359,  0.2570],\n",
      "        [ 0.0098, -0.1342, -0.1806, -0.1474, -0.2946, -0.0022, -0.1237, -0.0409,\n",
      "         -0.2970, -0.1228],\n",
      "        [-0.1322, -0.2492,  0.2112,  0.1821,  0.1686, -0.0627, -0.2576,  0.1519,\n",
      "         -0.2899, -0.2020],\n",
      "        [ 0.3082,  0.2646,  0.0199, -0.1126,  0.2477, -0.1511, -0.2188, -0.0499,\n",
      "         -0.3085,  0.3031]])\n",
      "base_model.model.2.lora_B.lora2.weight Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name in [\"base_model.model.0.lora_A.lora1.weight\", \"base_model.model.0.lora_B.lora1.weight\"]:\n",
    "        param.data = torch.ones_like(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-364.4426, -253.1330]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.arange(0, 10).view(1, 10).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_adapter(\"lora2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lora2']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.active_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9952,  1.1457]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.arange(0, 10).view(1, 10).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、禁用适配器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_adapter(\"lora1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-364.4426, -253.1330]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.arange(0, 10).view(1, 10).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9952,  1.1457]])\n"
     ]
    }
   ],
   "source": [
    "with model.disable_adapter():\n",
    "    print(model(torch.arange(0, 10).view(1, 10).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "4b9a2058-c011-43b7-9c58-7c70a63c6713",
      "metadata": {},
      "source": [
       "## 文本相似度text_similarity----cross-model交互策略\n",
       "本质上也是分类任务"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "6ea1060d-cd10-4b35-8d24-b4e9135604d4",
      "metadata": {},
      "source": [
       "## Step1 导入相关包"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 1,
      "id": "81c8a007-ff35-4879-9a0c-a7cafbeb29ae",
      "metadata": {},
      "outputs": [],
      "source": [
       "import os\n",
       "\n",
       "# 设置可见的 GPU\n",
       "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
       "\n",
       "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
       "from datasets import load_dataset"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "9c0753af-0169-4033-a0df-372ff2ff2a40",
      "metadata": {},
      "source": [
       "## Step2 加载数据集"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dfae56ab-f3fd-408a-89c7-985e563594dc",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "Dataset({\n",
          "    features: ['sentence1', 'sentence2', 'label'],\n",
          "    num_rows: 10000\n",
          "})"
         ]
        },
        "execution_count": 2,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dataset = load_dataset(\"json\", data_files=\"train_pair_1w.json\", split=\"train\")\n",
       "dataset"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "2d0c5a93-8e3f-430c-ad19-b5d65c580364",
      "metadata": {},
      "source": [
       "## Step3 划分数据集\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "6be6d5b6-7ead-4bba-ad05-ea2c54f19c8b",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "DatasetDict({\n",
          "    train: Dataset({\n",
          "        features: ['sentence1', 'sentence2', 'label'],\n",
          "        num_rows: 8000\n",
          "    })\n",
          "    test: Dataset({\n",
          "        features: ['sentence1', 'sentence2', 'label'],\n",
          "        num_rows: 2000\n",
          "    })\n",
          "})"
         ]
        },
        "execution_count": 3,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "datasets = dataset.train_test_split(test_size=0.2)\n",
       "datasets"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "a69bec23-2651-428b-96ec-654f2cd77c22",
      "metadata": {},
      "source": [
       "## Step4 数据集预处理"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6d581401-21b0-4257-991b-938624f04933",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "application/vnd.jupyter.widget-view+json": {
          "model_id": "8bc93fe872e54ba0821a4429a1aa18e0",
          "version_major": 2,
          "version_minor": 0
         },
         "text/plain": [
          "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
         ]
        },
        "metadata": {},
        "output_type": "display_data"
       },
       {
        "data": {
         "application/vnd.jupyter.widget-view+json": {
          "model_id": "b49186681a474332a2e58ddd5690a7b3",
          "version_major": 2,
          "version_minor": 0
         },
         "text/plain": [
          "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
         ]
        },
        "metadata": {},
        "output_type": "display_data"
       },
       {
        "data": {
         "text/plain": [
          "DatasetDict({\n",
          "    train: Dataset({\n",
          "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
          "        num_rows: 8000\n",
          "    })\n",
          "    test: Dataset({\n",
          "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
          "        num_rows: 2000\n",
          "    })\n",
          "})"
         ]
        },
        "execution_count": 4,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "import torch\n",
       "\n",
       "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")\n",
       "\n",
       "def process_function(examples):\n",
       "    tokenized_examples = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], max_length=128, truncation=True)\n",
       "    tokenized_examples[\"labels\"] = [float(label) for label in examples[\"label\"]]\n",
       "    return tokenized_examples\n",
       "\n",
       "tokenized_datasets = datasets.map(process_function, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
       "tokenized_datasets"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d8f9f755",
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "{'input_ids': [101, 4511, 2094, 6716, 4959, 3340, 5292, 6137, 6136, 8024, 1381, 5504, 2806, 3312, 5074, 102, 1343, 3378, 702, 1765, 3175, 4638, 782, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0.0}\n"
        ]
       }
      ],
      "source": [
       "print(tokenized_datasets[\"train\"][0])"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "1442c955-5581-4ba0-9a36-9142d3878975",
      "metadata": {},
      "source": [
       "## Step5 创建模型"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 6,
      "id": "21d35e38-55fa-4ff6-8305-246ef12a3426",
      "metadata": {},
      "outputs": [
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
         "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        ]
       }
      ],
      "source": [
       "model = AutoModelForSequenceClassification.from_pretrained(\"hfl/chinese-macbert-base\", num_labels=1)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "a6da8896",
      "metadata": {},
      "source": [
       "## Step6 创建评估函数"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2efd717c",
      "metadata": {},
      "outputs": [],
      "source": [
       "import evaluate\n",
       "\n",
       "acc_metric = evaluate.load(\"accuracy\")\n",
       "f1_metric = evaluate.load(\"f1\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a52487d3",
      "metadata": {},
      "outputs": [],
      "source": [
       "def eval_metric(eval_predict):\n",
       "    predictions, labels = eval_predict\n",
       "    predictions = [1 if p > 0.5 else 0 for p in predictions]\n",
       "    labels = [int(label) for label in labels]\n",
       "    # predictions = predictions.argmax(axis=-1)\n",
       "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
       "    f1 = f1_metric.compute(predictions=predictions, references=labels)\n",
       "    acc.update(f1)\n",
       "    return acc"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "8bba5a66",
      "metadata": {},
      "source": [
       "## Step7 创建TrainArguments"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d6c47a08",
      "metadata": {},
      "outputs": [
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "/node6_1/tanshuai/.conda/envs/abc/lib/python3.9/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
         "  warnings.warn(\n",
         "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
         "To disable this warning, you can either:\n",
         "\t- Avoid using `tokenizers` before the fork if possible\n",
         "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
         "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
         "To disable this warning, you can either:\n",
         "\t- Avoid using `tokenizers` before the fork if possible\n",
         "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
         "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
         "To disable this warning, you can either:\n",
         "\t- Avoid using `tokenizers` before the fork if possible\n",
         "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
        ]
       },
       {
        "data": {
         "text/plain": [
          "TrainingArguments(\n",
          "_n_gpu=1,\n",
          "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
          "adafactor=False,\n",
          "adam_beta1=0.9,\n",
          "adam_beta2=0.999,\n",
          "adam_epsilon=1e-08,\n",
          "auto_find_batch_size=False,\n",
          "batch_eval_metrics=False,\n",
          "bf16=False,\n",
          "bf16_full_eval=False,\n",
          "data_seed=None,\n",
          "dataloader_drop_last=False,\n",
          "dataloader_num_workers=0,\n",
          "dataloader_persistent_workers=False,\n",
          "dataloader_pin_memory=True,\n",
          "dataloader_prefetch_factor=None,\n",
          "ddp_backend=None,\n",
          "ddp_broadcast_buffers=None,\n",
          "ddp_bucket_cap_mb=None,\n",
          "ddp_find_unused_parameters=None,\n",
          "ddp_timeout=1800,\n",
          "debug=[],\n",
          "deepspeed=None,\n",
          "disable_tqdm=False,\n",
          "dispatch_batches=None,\n",
          "do_eval=True,\n",
          "do_predict=False,\n",
          "do_train=False,\n",
          "eval_accumulation_steps=None,\n",
          "eval_delay=0,\n",
          "eval_do_concat_batches=True,\n",
          "eval_on_start=False,\n",
          "eval_steps=None,\n",
          "eval_strategy=epoch,\n",
          "evaluation_strategy=epoch,\n",
          "fp16=False,\n",
          "fp16_backend=auto,\n",
          "fp16_full_eval=False,\n",
          "fp16_opt_level=O1,\n",
          "fsdp=[],\n",
          "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
          "fsdp_min_num_params=0,\n",
          "fsdp_transformer_layer_cls_to_wrap=None,\n",
          "full_determinism=False,\n",
          "gradient_accumulation_steps=1,\n",
          "gradient_checkpointing=False,\n",
          "gradient_checkpointing_kwargs=None,\n",
          "greater_is_better=True,\n",
          "group_by_length=False,\n",
          "half_precision_backend=auto,\n",
          "hub_always_push=False,\n",
          "hub_model_id=None,\n",
          "hub_private_repo=False,\n",
          "hub_strategy=every_save,\n",
          "hub_token=<HUB_TOKEN>,\n",
          "ignore_data_skip=False,\n",
          "include_inputs_for_metrics=False,\n",
          "include_num_input_tokens_seen=False,\n",
          "include_tokens_per_second=False,\n",
          "jit_mode_eval=False,\n",
          "label_names=None,\n",
          "label_smoothing_factor=0.0,\n",
          "learning_rate=2e-05,\n",
          "length_column_name=length,\n",
          "load_best_model_at_end=True,\n",
          "local_rank=0,\n",
          "log_level=passive,\n",
          "log_level_replica=warning,\n",
          "log_on_each_node=True,\n",
          "logging_dir=./cross_model/runs/Oct31_10-52-56_node6,\n",
          "logging_first_step=False,\n",
          "logging_nan_inf_filter=True,\n",
          "logging_steps=10,\n",
          "logging_strategy=steps,\n",
          "lr_scheduler_kwargs={},\n",
          "lr_scheduler_type=linear,\n",
          "max_grad_norm=1.0,\n",
          "max_steps=-1,\n",
          "metric_for_best_model=f1,\n",
          "mp_parameters=,\n",
          "neftune_noise_alpha=None,\n",
          "no_cuda=False,\n",
          "num_train_epochs=3.0,\n",
          "optim=adamw_torch,\n",
          "optim_args=None,\n",
          "optim_target_modules=None,\n",
          "output_dir=./cross_model,\n",
          "overwrite_output_dir=False,\n",
          "past_index=-1,\n",
          "per_device_eval_batch_size=32,\n",
          "per_device_train_batch_size=32,\n",
          "prediction_loss_only=False,\n",
          "push_to_hub=False,\n",
          "push_to_hub_model_id=None,\n",
          "push_to_hub_organization=None,\n",
          "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
          "ray_scope=last,\n",
          "remove_unused_columns=True,\n",
          "report_to=['tensorboard'],\n",
          "restore_callback_states_from_checkpoint=False,\n",
          "resume_from_checkpoint=None,\n",
          "run_name=./cross_model,\n",
          "save_on_each_node=False,\n",
          "save_only_model=False,\n",
          "save_safetensors=True,\n",
          "save_steps=500,\n",
          "save_strategy=epoch,\n",
          "save_total_limit=2,\n",
          "seed=42,\n",
          "skip_memory_metrics=True,\n",
          "split_batches=None,\n",
          "tf32=None,\n",
          "torch_compile=False,\n",
          "torch_compile_backend=None,\n",
          "torch_compile_mode=None,\n",
          "torchdynamo=None,\n",
          "tpu_metrics_debug=False,\n",
          "tpu_num_cores=None,\n",
          "use_cpu=False,\n",
          "use_ipex=False,\n",
          "use_legacy_prediction_loop=False,\n",
          "use_mps_device=False,\n",
          "warmup_ratio=0.0,\n",
          "warmup_steps=0,\n",
          "weight_decay=0.01,\n",
          ")"
         ]
        },
        "execution_count": 9,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "train_args = TrainingArguments(output_dir=\"./cross_model\",      # 输出文件夹\n",
       "                               per_device_train_batch_size=32,  # 训练时的batch_size\n",
       "                               per_device_eval_batch_size=32,  # 验证时的batch_size\n",
       "                               logging_steps=10,                # log 打印的频率\n",
       "                               evaluation_strategy=\"epoch\",     # 评估策略\n",
       "                               save_strategy=\"epoch\",           # 保存策略\n",
       "                               save_total_limit=2,              # 最大保存数\n",
       "                               learning_rate=2e-5,              # 学习率\n",
       "                               weight_decay=0.01,               # weight_decay\n",
       "                               metric_for_best_model=\"f1\",      # 设定评估指标\n",
       "                               load_best_model_at_end=True      # 训练完成后加载最优模型\n",
       "                               )     \n",
       "train_args"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "d12ab3de",
      "metadata": {},
      "source": [
       "## Step8 创建Trainer"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1e099a67",
      "metadata": {},
      "outputs": [],
      "source": [
       "from transformers import DataCollatorWithPadding\n",
       "trainer = Trainer(model=model, \n",
       "                  args=train_args, \n",
       "                  train_dataset=tokenized_datasets[\"train\"], \n",
       "                  eval_dataset=tokenized_datasets[\"test\"], \n",
       "                  data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
       "                  compute_metrics=eval_metric)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "7139c66d-f2cb-405c-916b-6f8302b8c0bc",
      "metadata": {},
      "source": [
       "## Step9 模型训练"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 11,
      "id": "22ef3fe9",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/html": [
          "\n",
          "    <div>\n",
          "      \n",
          "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
          "      [750/750 02:26, Epoch 3/3]\n",
          "    </div>\n",
          "    <table border=\"1\" class=\"dataframe\">\n",
          "  <thead>\n",
          " <tr style=\"text-align: left;\">\n",
          "      <th>Epoch</th>\n",
          "      <th>Training Loss</th>\n",
          "      <th>Validation Loss</th>\n",
          "      <th>Accuracy</th>\n",
          "      <th>F1</th>\n",
          "    </tr>\n",
          "  </thead>\n",
          "  <tbody>\n",
          "    <tr>\n",
          "      <td>1</td>\n",
          "      <td>0.109900</td>\n",
          "      <td>0.069796</td>\n",
          "      <td>0.907000</td>\n",
          "      <td>0.883750</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <td>2</td>\n",
          "      <td>0.074100</td>\n",
          "      <td>0.061060</td>\n",
          "      <td>0.918500</td>\n",
          "      <td>0.898569</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <td>3</td>\n",
          "      <td>0.064400</td>\n",
          "      <td>0.063728</td>\n",
          "      <td>0.920000</td>\n",
          "      <td>0.900374</td>\n",
          "    </tr>\n",
          "  </tbody>\n",
          "</table><p>"
         ],
         "text/plain": [
          "<IPython.core.display.HTML object>"
         ]
        },
        "metadata": {},
        "output_type": "display_data"
       },
       {
        "data": {
         "text/plain": [
          "TrainOutput(global_step=750, training_loss=0.08847727115948995, metrics={'train_runtime': 147.831, 'train_samples_per_second': 162.348, 'train_steps_per_second': 5.073, 'total_flos': 1553919940810752.0, 'train_loss': 0.08847727115948995, 'epoch': 3.0})"
         ]
        },
        "execution_count": 11,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "trainer.train()"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "930f1bd9",
      "metadata": {},
      "source": [
       "## Step10 模型评估"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 12,
      "id": "32304106",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/html": [
          "\n",
          "    <div>\n",
          "      \n",
          "      <progress value='1' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
          "      [ 1/63 : < :]\n",
          "    </div>\n",
          "    "
         ],
         "text/plain": [
          "<IPython.core.display.HTML object>"
         ]
        },
        "metadata": {},
        "output_type": "display_data"
       },
       {
        "data": {
         "text/plain": [
          "{'eval_loss': 0.06372786313295364,\n",
          " 'eval_accuracy': 0.92,\n",
          " 'eval_f1': 0.900373599003736,\n",
          " 'eval_runtime': 3.373,\n",
          " 'eval_samples_per_second': 592.945,\n",
          " 'eval_steps_per_second': 18.678,\n",
          " 'epoch': 3.0}"
         ]
        },
        "execution_count": 12,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "trainer.evaluate(tokenized_datasets[\"test\"])"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ecf7fadc",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'eval_loss': 0.020500192418694496,\n",
          " 'eval_accuracy': 0.97725,\n",
          " 'eval_f1': 0.9713295526149969,\n",
          " 'eval_runtime': 13.566,\n",
          " 'eval_samples_per_second': 589.711,\n",
          " 'eval_steps_per_second': 18.428,\n",
          " 'epoch': 3.0}"
         ]
        },
        "execution_count": 13,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "trainer.evaluate(tokenized_datasets[\"train\"])"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "7b7e198e-37f9-4d55-9df3-f2fa0b8cb894",
      "metadata": {},
      "source": [
       "## Step11 模型预测"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 32,
      "id": "ab053c96-8aa2-41da-9631-99ea69dc3db2",
      "metadata": {},
      "outputs": [],
      "source": [
       "from transformers import pipeline"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 33,
      "id": "7198df92",
      "metadata": {},
      "outputs": [],
      "source": [
       "model.config.id2label = {0:\"不相似\", 1:\"相似\"}"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 34,
      "id": "0e346126",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x7fe58306d8b0>"
         ]
        },
        "execution_count": 34,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "pipeline = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0)\n",
       "pipeline"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 49,
      "id": "6143ab32",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'label': '不相似', 'score': -0.004505789838731289}"
         ]
        },
        "execution_count": 49,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "result = pipeline({\"text\": \"我喜欢北京。\", \"text_pair\": \"天气怎样？\"}, function_to_apply=\"none\")\n",
       "result[\"label\"] = \"相似\" if result[\"score\"] > 0.5 else \"不相似\"\n",
       "result"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 48,
      "id": "2129c7ad",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'label': '相似', 'score': 1.0234427452087402}"
         ]
        },
        "execution_count": 48,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "result = pipeline({\"text\": \"我喜欢北京。\", \"text_pair\": \"北京是我喜欢的地方。\"}, function_to_apply=\"none\")\n",
       "result[\"label\"] = \"相似\" if result[\"score\"] > 0.5 else \"不相似\"\n",
       "result"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "993a65ec",
      "metadata": {},
      "source": [
       "## 训练过程可视化\n",
       "1、终端进入abc的conda环境和checkpoints目录，执行tensorboard --logdir=runs --host=0.0.0.0 --port=8418\n",
       "\n",
       "2、vscode中ctrl+shift+p，搜索TensorBoard"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "d509aa9b-cf71-414a-bdcc-6b374fa7ccda",
      "metadata": {},
      "outputs": [],
      "source": []
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   